{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import scipy.sparse as sp\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset for 4D Noisy Matrix\n",
    "class NoisyMatrixDataset(Dataset):\n",
    "    def __init__(self, original_matrix, noise_level=0.1, num_samples=1000):\n",
    "        self.original_matrix = original_matrix\n",
    "        self.noise_level = noise_level\n",
    "        self.num_samples = num_samples\n",
    "        self.data = self.generate_noisy_samples()\n",
    "\n",
    "    def generate_noisy_samples(self):\n",
    "        noisy_samples = []\n",
    "        for _ in range(self.num_samples):\n",
    "            noisy_sample = self.original_matrix.clone()\n",
    "            noise = torch.rand_like(noisy_sample)\n",
    "            mask = noise < self.noise_level\n",
    "            noisy_sample[mask] = 1 - noisy_sample[mask]  # Flip bits where mask is True (0 to 1 or 1 to 0)\n",
    "            noisy_samples.append(noisy_sample)\n",
    "        return torch.stack(noisy_samples)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx].view(-1), self.original_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Encoder\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, z_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc_mu = nn.Linear(hidden_dim, z_dim)       # Mean of latent variable\n",
    "        self.fc_logvar = nn.Linear(hidden_dim, z_dim)   # Log variance of latent variable\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = torch.relu(self.fc1(x))\n",
    "        mu = self.fc_mu(h)\n",
    "        logvar = self.fc_logvar(h)\n",
    "        return mu, logvar\n",
    "\n",
    "# Define the Decoder\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, z_dim, hidden_dim, output_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(z_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, z):\n",
    "        h = torch.relu(self.fc1(z))\n",
    "        x_reconstructed = torch.sigmoid(self.fc2(h))\n",
    "        return x_reconstructed\n",
    "\n",
    "# Define the VAE\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, z_dim):\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder = Encoder(input_dim, hidden_dim, z_dim)\n",
    "        self.decoder = Decoder(z_dim, hidden_dim, input_dim)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encoder(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        x_reconstructed = self.decoder(z)\n",
    "        return x_reconstructed, mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function for VAE\n",
    "def vae_loss(x, x_reconstructed, mu, logvar):\n",
    "    reconstruction_loss = nn.functional.binary_cross_entropy(x_reconstructed, x, reduction='sum')\n",
    "    kl_divergence = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return reconstruction_loss + kl_divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate initial 4D matrix and create dataset\n",
    "initial_matrix = torch.randint(0, 2, (10, 4, 5, 6)).float()  # Example initial 4D binary matrix\n",
    "noise_level = 0.1\n",
    "num_samples = 10000\n",
    "dataset = NoisyMatrixDataset(initial_matrix, noise_level=noise_level, num_samples=num_samples)\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1200"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model parameters\n",
    "input_dim = initial_matrix.numel()\n",
    "hidden_dim = 400\n",
    "z_dim = 20\n",
    "input_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set specific GPU device if available\n",
    "gpu_device = 0  # Change this value to set a specific GPU device\n",
    "device = torch.device(f'cuda:{gpu_device}' if torch.cuda.is_available() else 'cpu')\n",
    "device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate model, optimizer, and training loop\n",
    "vae = VAE(input_dim, hidden_dim, z_dim).to(device)\n",
    "optimizer = optim.Adam(vae.parameters(), lr=1e-3, weight_decay=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 6315.6152\n",
      "Epoch [2/10], Loss: 6500.3955\n",
      "Epoch [3/10], Loss: 6426.1162\n",
      "Epoch [4/10], Loss: 6233.6528\n",
      "Epoch [5/10], Loss: 6208.0840\n",
      "Epoch [6/10], Loss: 6133.7461\n",
      "Epoch [7/10], Loss: 6305.9321\n",
      "Epoch [8/10], Loss: 6136.2178\n",
      "Epoch [9/10], Loss: 6167.4648\n",
      "Epoch [10/10], Loss: 6279.9175\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    for batch_idx, (data, _) in enumerate(dataloader):\n",
    "        data = data.to(device).float()\n",
    "        optimizer.zero_grad()\n",
    "        x_reconstructed, mu, logvar = vae(data)\n",
    "        loss = vae_loss(data, x_reconstructed, mu, logvar)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
